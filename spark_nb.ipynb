{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a8ef2b3-e973-44fe-9faf-8b7569cd2184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenBLAS WARNING - could not determine the L2 cache size on this system, assuming 256k\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "128b6ac8-fcdd-4187-a16a-24f775178667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-30 18:25:22,903 INFO spark.SparkContext: Running Spark version 3.4.1\n",
      "2023-10-30 18:25:23,255 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2023-10-30 18:25:23,408 INFO resource.ResourceUtils: ==============================================================\n",
      "2023-10-30 18:25:23,409 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\n",
      "2023-10-30 18:25:23,409 INFO resource.ResourceUtils: ==============================================================\n",
      "2023-10-30 18:25:23,409 INFO spark.SparkContext: Submitted application: spark-nb\n",
      "2023-10-30 18:25:23,479 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "2023-10-30 18:25:23,499 INFO resource.ResourceProfile: Limiting resource is cpus at 1 tasks per executor\n",
      "2023-10-30 18:25:23,503 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\n",
      "2023-10-30 18:25:23,648 INFO spark.SecurityManager: Changing view acls to: root\n",
      "2023-10-30 18:25:23,648 INFO spark.SecurityManager: Changing modify acls to: root\n",
      "2023-10-30 18:25:23,649 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2023-10-30 18:25:23,649 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2023-10-30 18:25:23,650 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
      "2023-10-30 18:25:24,398 INFO util.Utils: Successfully started service 'sparkDriver' on port 46119.\n",
      "2023-10-30 18:25:24,479 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "2023-10-30 18:25:24,575 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "2023-10-30 18:25:24,669 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2023-10-30 18:25:24,670 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "2023-10-30 18:25:24,677 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "2023-10-30 18:25:24,762 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-0bb79a08-c259-4bae-b09d-65055f2d5e69\n",
      "2023-10-30 18:25:24,805 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
      "2023-10-30 18:25:24,841 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "2023-10-30 18:25:24,994 INFO util.log: Logging initialized @5335ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "2023-10-30 18:25:25,228 INFO ui.JettyUtils: Start Jetty 0.0.0.0:4051 for SparkUI\n",
      "2023-10-30 18:25:25,268 INFO server.Server: jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 1.8.0_382-b05\n",
      "2023-10-30 18:25:25,315 INFO server.Server: Started @5659ms\n",
      "2023-10-30 18:25:25,463 INFO server.AbstractConnector: Started ServerConnector@2c536ca7{HTTP/1.1, (http/1.1)}{0.0.0.0:4051}\n",
      "2023-10-30 18:25:25,464 INFO util.Utils: Successfully started service 'SparkUI' on port 4051.\n",
      "2023-10-30 18:25:25,527 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@44478bdc{/,null,AVAILABLE,@Spark}\n",
      "2023-10-30 18:25:25,883 INFO client.StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...\n",
      "2023-10-30 18:25:26,073 INFO client.TransportClientFactory: Successfully created connection to spark-master/172.20.0.5:7077 after 126 ms (0 ms spent in bootstraps)\n",
      "2023-10-30 18:25:26,271 INFO cluster.StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20231030182526-0001\n",
      "2023-10-30 18:25:26,283 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44443.\n",
      "2023-10-30 18:25:26,283 INFO netty.NettyBlockTransferService: Server created on 9dc5d8af21bf:44443\n",
      "2023-10-30 18:25:26,286 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2023-10-30 18:25:26,310 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 9dc5d8af21bf, 44443, None)\n",
      "2023-10-30 18:25:26,318 INFO storage.BlockManagerMasterEndpoint: Registering block manager 9dc5d8af21bf:44443 with 366.3 MiB RAM, BlockManagerId(driver, 9dc5d8af21bf, 44443, None)\n",
      "2023-10-30 18:25:26,323 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 9dc5d8af21bf, 44443, None)\n",
      "2023-10-30 18:25:26,325 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 9dc5d8af21bf, 44443, None)\n",
      "2023-10-30 18:25:27,123 INFO history.SingleEventLogFileWriter: Logging events to file:/home/spark-events/app-20231030182526-0001.inprogress\n",
      "2023-10-30 18:25:27,435 INFO util.Utils: Using initial executors = 0, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "2023-10-30 18:25:27,436 INFO spark.ExecutorAllocationManager: Dynamic allocation is enabled without a shuffle service.\n",
      "2023-10-30 18:25:27,511 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@44478bdc{/,null,STOPPED,@Spark}\n",
      "2023-10-30 18:25:27,512 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@14dac96a{/jobs,null,AVAILABLE,@Spark}\n",
      "2023-10-30 18:25:27,513 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2d502ac6{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2023-10-30 18:25:27,514 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5973ea77{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2023-10-30 18:25:27,516 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@73d267df{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2023-10-30 18:25:27,517 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@69c168b8{/stages,null,AVAILABLE,@Spark}\n",
      "2023-10-30 18:25:27,517 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@33c27cf9{/stages/json,null,AVAILABLE,@Spark}\n",
      "2023-10-30 18:25:27,518 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@45a9a9de{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2023-10-30 18:25:27,519 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@16582692{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2023-10-30 18:25:27,519 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1a3e11a6{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2023-10-30 18:25:27,520 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@769e2878{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2023-10-30 18:25:27,524 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e26695{/storage,null,AVAILABLE,@Spark}\n",
      "2023-10-30 18:25:27,525 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@aa0f218{/storage/json,null,AVAILABLE,@Spark}\n",
      "2023-10-30 18:25:27,525 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2ca2dfff{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2023-10-30 18:25:27,527 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7a8ff3ee{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2023-10-30 18:25:27,527 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5a976f51{/environment,null,AVAILABLE,@Spark}\n",
      "2023-10-30 18:25:27,528 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3fa94464{/environment/json,null,AVAILABLE,@Spark}\n",
      "2023-10-30 18:25:27,530 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3ef94f6f{/executors,null,AVAILABLE,@Spark}\n",
      "2023-10-30 18:25:27,532 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@17e4a40d{/executors/json,null,AVAILABLE,@Spark}\n",
      "2023-10-30 18:25:27,538 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@16e37dba{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2023-10-30 18:25:27,539 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1c04fd81{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2023-10-30 18:25:27,579 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5dfbb3ed{/static,null,AVAILABLE,@Spark}\n",
      "2023-10-30 18:25:27,581 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64b710d2{/,null,AVAILABLE,@Spark}\n",
      "2023-10-30 18:25:27,587 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7fef6931{/api,null,AVAILABLE,@Spark}\n",
      "2023-10-30 18:25:27,588 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@461638ea{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2023-10-30 18:25:27,589 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@d571685{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2023-10-30 18:25:27,596 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@375405cd{/metrics/json,null,AVAILABLE,@Spark}\n",
      "2023-10-30 18:25:27,597 INFO cluster.StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('spark-nb') \\\n",
    "    .master('spark://spark-master:7077') \\\n",
    "    .config('spark.ui.port', '4051') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecccef5d-a75e-45b3-a75f-0ec34326d02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-10-30 18:25:28--  https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-01.parquet\n",
      "Resolving d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 18.244.115.107, 18.244.115.167, 18.244.115.202, ...\n",
      "Connecting to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|18.244.115.107|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1427002 (1.4M) [binary/octet-stream]\n",
      "Saving to: '/home/data/nyc_taxi_data/green_tripdata/green_tripdata_2023-01.parquet'\n",
      "\n",
      "100%[======================================>] 1,427,002   1.22MB/s   in 1.1s   \n",
      "\n",
      "2023-10-30 18:25:31 (1.22 MB/s) - '/home/data/nyc_taxi_data/green_tripdata/green_tripdata_2023-01.parquet' saved [1427002/1427002]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p /home/data/nyc_taxi_data/green_tripdata && \\\n",
    "    rm -f /home/data/nyc_taxi_data/green_tripdata/* && \\\n",
    "    wget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-01.parquet -P /home/data/nyc_taxi_data/green_tripdata/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "459e9a63-de73-4404-8544-945e33cb0a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-30 18:25:31,763 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "2023-10-30 18:25:31,768 INFO internal.SharedState: Warehouse path is 'file:/home/spark-warehouse'.\n",
      "2023-10-30 18:25:31,800 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6558dea2{/SQL,null,AVAILABLE,@Spark}\n",
      "2023-10-30 18:25:31,801 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@c0e13fd{/SQL/json,null,AVAILABLE,@Spark}\n",
      "2023-10-30 18:25:31,802 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3edbddf8{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "2023-10-30 18:25:31,803 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@56caab05{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "2023-10-30 18:25:31,809 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4de71bcd{/static/sql,null,AVAILABLE,@Spark}\n",
      "2023-10-30 18:25:34,480 INFO datasources.InMemoryFileIndex: It took 59 ms to list leaf files for 1 paths.\n",
      "2023-10-30 18:25:35,167 INFO spark.SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0\n",
      "2023-10-30 18:25:35,199 INFO scheduler.DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-10-30 18:25:35,200 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)\n",
      "2023-10-30 18:25:35,200 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-10-30 18:25:35,201 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-10-30 18:25:35,208 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at load at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-10-30 18:25:35,374 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 152.3 KiB, free 366.2 MiB)\n",
      "2023-10-30 18:25:35,493 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 56.1 KiB, free 366.1 MiB)\n",
      "2023-10-30 18:25:35,497 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 9dc5d8af21bf:44443 (size: 56.1 KiB, free: 366.2 MiB)\n",
      "2023-10-30 18:25:35,508 INFO spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535\n",
      "2023-10-30 18:25:35,536 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-10-30 18:25:35,539 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "2023-10-30 18:25:36,306 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20231030182526-0001/0 on worker-20231030182229-172.20.0.4-36089 (172.20.0.4:36089) with 1 core(s)\n",
      "2023-10-30 18:25:36,308 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20231030182526-0001/0 on hostPort 172.20.0.4:36089 with 1 core(s), 1024.0 MiB RAM\n",
      "2023-10-30 18:25:36,297 INFO spark.ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1 for resource profile id: 0)\n",
      "2023-10-30 18:25:36,374 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20231030182526-0001/0 is now RUNNING\n",
      "2023-10-30 18:25:42,272 INFO cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.20.0.4:55392) with ID 0,  ResourceProfileId 0\n",
      "2023-10-30 18:25:42,286 INFO dynalloc.ExecutorMonitor: New executor 0 has registered (new total is 1)\n",
      "2023-10-30 18:25:42,600 INFO storage.BlockManagerMasterEndpoint: Registering block manager 172.20.0.4:39963 with 366.3 MiB RAM, BlockManagerId(0, 172.20.0.4, 39963, None)\n",
      "2023-10-30 18:25:42,824 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.20.0.4, executor 0, partition 0, PROCESS_LOCAL, 7537 bytes) \n",
      "2023-10-30 18:25:43,540 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.20.0.4:39963 (size: 56.1 KiB, free: 366.2 MiB)\n",
      "2023-10-30 18:25:46,373 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3572 ms on 172.20.0.4 (executor 0) (1/1)\n",
      "2023-10-30 18:25:46,378 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "2023-10-30 18:25:46,394 INFO scheduler.DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 11.146 s\n",
      "2023-10-30 18:25:46,402 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-10-30 18:25:46,402 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "2023-10-30 18:25:46,407 INFO scheduler.DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 11.239482 s\n",
      "2023-10-30 18:25:46,598 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 9dc5d8af21bf:44443 in memory (size: 56.1 KiB, free: 366.3 MiB)\n",
      "2023-10-30 18:25:46,624 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 172.20.0.4:39963 in memory (size: 56.1 KiB, free: 366.3 MiB)\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('parquet').load('/home/data/nyc_taxi_data/green_tripdata/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ec0b095-0002-450c-aeb4-9b5ef66570c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-30 18:25:52,827 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-10-30 18:25:52,828 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2023-10-30 18:25:54,052 INFO codegen.CodeGenerator: Code generated in 663.514635 ms\n",
      "2023-10-30 18:25:54,132 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 487.1 KiB, free 365.8 MiB)\n",
      "2023-10-30 18:25:54,147 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 53.7 KiB, free 365.8 MiB)\n",
      "2023-10-30 18:25:54,147 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 9dc5d8af21bf:44443 (size: 53.7 KiB, free: 366.2 MiB)\n",
      "2023-10-30 18:25:54,149 INFO spark.SparkContext: Created broadcast 1 from count at NativeMethodAccessorImpl.java:0\n",
      "2023-10-30 18:25:54,176 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-10-30 18:25:54,348 INFO scheduler.DAGScheduler: Registering RDD 5 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "2023-10-30 18:25:54,360 INFO scheduler.DAGScheduler: Got map stage job 1 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-10-30 18:25:54,361 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 1 (count at NativeMethodAccessorImpl.java:0)\n",
      "2023-10-30 18:25:54,361 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-10-30 18:25:54,362 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-10-30 18:25:54,364 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-10-30 18:25:54,418 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 15.6 KiB, free 365.8 MiB)\n",
      "2023-10-30 18:25:54,420 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 365.7 MiB)\n",
      "2023-10-30 18:25:54,421 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 9dc5d8af21bf:44443 (size: 7.4 KiB, free: 366.2 MiB)\n",
      "2023-10-30 18:25:54,422 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535\n",
      "2023-10-30 18:25:54,424 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-10-30 18:25:54,424 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "2023-10-30 18:25:54,430 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.20.0.4, executor 0, partition 0, PROCESS_LOCAL, 7950 bytes) \n",
      "2023-10-30 18:25:54,516 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.20.0.4:39963 (size: 7.4 KiB, free: 366.3 MiB)\n",
      "2023-10-30 18:25:55,849 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.20.0.4:39963 (size: 53.7 KiB, free: 366.2 MiB)\n",
      "2023-10-30 18:25:56,448 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2023 ms on 172.20.0.4 (executor 0) (1/1)\n",
      "2023-10-30 18:25:56,448 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "2023-10-30 18:25:56,450 INFO scheduler.DAGScheduler: ShuffleMapStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 2.078 s\n",
      "2023-10-30 18:25:56,451 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-10-30 18:25:56,451 INFO scheduler.DAGScheduler: running: Set()\n",
      "2023-10-30 18:25:56,451 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-10-30 18:25:56,452 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-10-30 18:25:56,530 INFO codegen.CodeGenerator: Code generated in 20.034735 ms\n",
      "2023-10-30 18:25:56,571 INFO spark.SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "2023-10-30 18:25:56,575 INFO scheduler.DAGScheduler: Got job 2 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-10-30 18:25:56,575 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (count at NativeMethodAccessorImpl.java:0)\n",
      "2023-10-30 18:25:56,575 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
      "2023-10-30 18:25:56,576 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-10-30 18:25:56,579 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[8] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-10-30 18:25:56,592 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 12.2 KiB, free 365.7 MiB)\n",
      "2023-10-30 18:25:56,594 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 365.7 MiB)\n",
      "2023-10-30 18:25:56,595 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 9dc5d8af21bf:44443 (size: 5.9 KiB, free: 366.2 MiB)\n",
      "2023-10-30 18:25:56,596 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n",
      "2023-10-30 18:25:56,598 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[8] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-10-30 18:25:56,598 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "2023-10-30 18:25:56,606 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (172.20.0.4, executor 0, partition 0, NODE_LOCAL, 7367 bytes) \n",
      "2023-10-30 18:25:56,635 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.20.0.4:39963 (size: 5.9 KiB, free: 366.2 MiB)\n",
      "2023-10-30 18:25:56,686 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.20.0.4:55392\n",
      "2023-10-30 18:25:56,898 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 294 ms on 172.20.0.4 (executor 0) (1/1)\n",
      "2023-10-30 18:25:56,899 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "2023-10-30 18:25:56,902 INFO scheduler.DAGScheduler: ResultStage 3 (count at NativeMethodAccessorImpl.java:0) finished in 0.313 s\n",
      "2023-10-30 18:25:56,902 INFO scheduler.DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-10-30 18:25:56,903 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "2023-10-30 18:25:56,903 INFO scheduler.DAGScheduler: Job 2 finished: count at NativeMethodAccessorImpl.java:0, took 0.331628 s\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "68211"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f51a756-fce4-4dcd-abbd-d2960771d36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- lpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- lpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- RatecodeID: double (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- ehail_fee: integer (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: double (nullable = true)\n",
      " |-- trip_type: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70e76a46-ca01-4373-b4e1-d2c7e0b779ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "# fix error in pandas 2.1.1\n",
    "# Casting to unit-less dtype 'datetime64' is not supported. Pass e.g. 'datetime64[ns]' instead.\n",
    "df = df.withColumn(\"lpep_pickup_datetime\", date_format(\"lpep_pickup_datetime\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "df = df.withColumn(\"lpep_dropoff_datetime\", date_format(\"lpep_dropoff_datetime\", \"yyyy-MM-dd HH:mm:ss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14c9b977-180c-4e2f-894f-1d17c08a4cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-30 18:25:58,179 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-10-30 18:25:58,179 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2023-10-30 18:25:58,354 INFO codegen.CodeGenerator: Code generated in 105.810509 ms\n",
      "2023-10-30 18:25:58,364 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 492.7 KiB, free 365.3 MiB)\n",
      "2023-10-30 18:25:58,374 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 54.6 KiB, free 365.2 MiB)\n",
      "2023-10-30 18:25:58,376 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 9dc5d8af21bf:44443 (size: 54.6 KiB, free: 366.2 MiB)\n",
      "2023-10-30 18:25:58,377 INFO spark.SparkContext: Created broadcast 4 from toPandas at /tmp/ipykernel_20/1309501217.py:1\n",
      "2023-10-30 18:25:58,378 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-10-30 18:25:58,402 INFO spark.SparkContext: Starting job: toPandas at /tmp/ipykernel_20/1309501217.py:1\n",
      "2023-10-30 18:25:58,403 INFO scheduler.DAGScheduler: Got job 3 (toPandas at /tmp/ipykernel_20/1309501217.py:1) with 1 output partitions\n",
      "2023-10-30 18:25:58,404 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (toPandas at /tmp/ipykernel_20/1309501217.py:1)\n",
      "2023-10-30 18:25:58,404 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-10-30 18:25:58,404 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-10-30 18:25:58,409 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[12] at toPandas at /tmp/ipykernel_20/1309501217.py:1), which has no missing parents\n",
      "2023-10-30 18:25:58,418 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 26.1 KiB, free 365.2 MiB)\n",
      "2023-10-30 18:25:58,420 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 365.2 MiB)\n",
      "2023-10-30 18:25:58,421 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 9dc5d8af21bf:44443 (size: 8.6 KiB, free: 366.2 MiB)\n",
      "2023-10-30 18:25:58,423 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535\n",
      "2023-10-30 18:25:58,423 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[12] at toPandas at /tmp/ipykernel_20/1309501217.py:1) (first 15 tasks are for partitions Vector(0))\n",
      "2023-10-30 18:25:58,423 INFO scheduler.TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "2023-10-30 18:25:58,425 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (172.20.0.4, executor 0, partition 0, PROCESS_LOCAL, 7961 bytes) \n",
      "2023-10-30 18:25:58,455 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.20.0.4:39963 (size: 8.6 KiB, free: 366.2 MiB)\n",
      "2023-10-30 18:25:58,587 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.20.0.4:39963 (size: 54.6 KiB, free: 366.2 MiB)\n",
      "2023-10-30 18:26:00,112 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 1687 ms on 172.20.0.4 (executor 0) (1/1)\n",
      "2023-10-30 18:26:00,112 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "2023-10-30 18:26:00,113 INFO scheduler.DAGScheduler: ResultStage 4 (toPandas at /tmp/ipykernel_20/1309501217.py:1) finished in 1.701 s\n",
      "2023-10-30 18:26:00,113 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-10-30 18:26:00,113 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
      "2023-10-30 18:26:00,115 INFO scheduler.DAGScheduler: Job 3 finished: toPandas at /tmp/ipykernel_20/1309501217.py:1, took 1.711918 s\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>lpep_pickup_datetime</th>\n",
       "      <th>lpep_dropoff_datetime</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>ehail_fee</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>trip_type</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-01 00:26:10</td>\n",
       "      <td>2023-01-01 00:37:11</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>166</td>\n",
       "      <td>143</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>14.9</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.18</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-01 00:51:03</td>\n",
       "      <td>2023-01-01 00:57:49</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24</td>\n",
       "      <td>43</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.81</td>\n",
       "      <td>10.7</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.84</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-01 00:35:12</td>\n",
       "      <td>2023-01-01 00:41:32</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>223</td>\n",
       "      <td>179</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.94</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.64</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-01 00:13:14</td>\n",
       "      <td>2023-01-01 00:19:03</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41</td>\n",
       "      <td>238</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.30</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-01 00:33:04</td>\n",
       "      <td>2023-01-01 00:39:02</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41</td>\n",
       "      <td>74</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.10</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-01 00:53:31</td>\n",
       "      <td>2023-01-01 01:11:04</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41</td>\n",
       "      <td>262</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.78</td>\n",
       "      <td>17.7</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.95</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-01 00:09:14</td>\n",
       "      <td>2023-01-01 00:26:39</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>181</td>\n",
       "      <td>45</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.80</td>\n",
       "      <td>19.1</td>\n",
       "      <td>3.75</td>\n",
       "      <td>1.5</td>\n",
       "      <td>4.85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-01 00:11:58</td>\n",
       "      <td>2023-01-01 00:24:55</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24</td>\n",
       "      <td>75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.88</td>\n",
       "      <td>14.2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.70</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-01 00:41:29</td>\n",
       "      <td>2023-01-01 00:46:26</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41</td>\n",
       "      <td>166</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.11</td>\n",
       "      <td>7.2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.70</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-01 00:50:32</td>\n",
       "      <td>2023-01-01 01:13:42</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24</td>\n",
       "      <td>140</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.22</td>\n",
       "      <td>24.7</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.95</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID lpep_pickup_datetime lpep_dropoff_datetime store_and_fwd_flag  \\\n",
       "0         2  2023-01-01 00:26:10   2023-01-01 00:37:11                  N   \n",
       "1         2  2023-01-01 00:51:03   2023-01-01 00:57:49                  N   \n",
       "2         2  2023-01-01 00:35:12   2023-01-01 00:41:32                  N   \n",
       "3         1  2023-01-01 00:13:14   2023-01-01 00:19:03                  N   \n",
       "4         1  2023-01-01 00:33:04   2023-01-01 00:39:02                  N   \n",
       "5         2  2023-01-01 00:53:31   2023-01-01 01:11:04                  N   \n",
       "6         1  2023-01-01 00:09:14   2023-01-01 00:26:39                  N   \n",
       "7         2  2023-01-01 00:11:58   2023-01-01 00:24:55                  N   \n",
       "8         2  2023-01-01 00:41:29   2023-01-01 00:46:26                  N   \n",
       "9         2  2023-01-01 00:50:32   2023-01-01 01:13:42                  N   \n",
       "\n",
       "   RatecodeID  PULocationID  DOLocationID  passenger_count  trip_distance  \\\n",
       "0         1.0           166           143              1.0           2.58   \n",
       "1         1.0            24            43              1.0           1.81   \n",
       "2         1.0           223           179              1.0           0.00   \n",
       "3         1.0            41           238              1.0           1.30   \n",
       "4         1.0            41            74              1.0           1.10   \n",
       "5         1.0            41           262              1.0           2.78   \n",
       "6         1.0           181            45              2.0           3.80   \n",
       "7         1.0            24            75              1.0           1.88   \n",
       "8         1.0            41           166              2.0           1.11   \n",
       "9         1.0            24           140              1.0           4.22   \n",
       "\n",
       "   fare_amount  extra  mta_tax  tip_amount  tolls_amount  ehail_fee  \\\n",
       "0         14.9   1.00      0.5        4.03           0.0        NaN   \n",
       "1         10.7   1.00      0.5        2.64           0.0        NaN   \n",
       "2          7.2   1.00      0.5        1.94           0.0        NaN   \n",
       "3          6.5   0.50      1.5        1.70           0.0        NaN   \n",
       "4          6.0   0.50      1.5        0.00           0.0        NaN   \n",
       "5         17.7   1.00      0.5        0.00           0.0        NaN   \n",
       "6         19.1   3.75      1.5        4.85           0.0        NaN   \n",
       "7         14.2   1.00      0.5        0.00           0.0        NaN   \n",
       "8          7.2   1.00      0.5        1.00           0.0        NaN   \n",
       "9         24.7   1.00      0.5        3.00           0.0        NaN   \n",
       "\n",
       "   improvement_surcharge  total_amount  payment_type  trip_type  \\\n",
       "0                    1.0         24.18           1.0        1.0   \n",
       "1                    1.0         15.84           1.0        1.0   \n",
       "2                    1.0         11.64           1.0        1.0   \n",
       "3                    1.0         10.20           1.0        1.0   \n",
       "4                    1.0          8.00           1.0        1.0   \n",
       "5                    1.0         22.95           2.0        1.0   \n",
       "6                    1.0         29.20           1.0        1.0   \n",
       "7                    1.0         16.70           2.0        1.0   \n",
       "8                    1.0         10.70           1.0        1.0   \n",
       "9                    1.0         32.95           1.0        1.0   \n",
       "\n",
       "   congestion_surcharge  \n",
       "0                  2.75  \n",
       "1                  0.00  \n",
       "2                  0.00  \n",
       "3                  0.00  \n",
       "4                  0.00  \n",
       "5                  2.75  \n",
       "6                  2.75  \n",
       "7                  0.00  \n",
       "8                  0.00  \n",
       "9                  2.75  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2a02d40-fb15-4e2e-934a-b82b82fec40f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-30 18:26:00,411 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 9dc5d8af21bf:44443 in memory (size: 8.6 KiB, free: 366.2 MiB)\n",
      "2023-10-30 18:26:00,417 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 172.20.0.4:39963 in memory (size: 8.6 KiB, free: 366.2 MiB)\n",
      "2023-10-30 18:26:00,430 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 9dc5d8af21bf:44443 in memory (size: 53.7 KiB, free: 366.2 MiB)\n",
      "2023-10-30 18:26:00,435 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 172.20.0.4:39963 in memory (size: 53.7 KiB, free: 366.2 MiB)\n",
      "2023-10-30 18:26:00,452 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 9dc5d8af21bf:44443 in memory (size: 5.9 KiB, free: 366.2 MiB)\n",
      "2023-10-30 18:26:00,461 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 172.20.0.4:39963 in memory (size: 5.9 KiB, free: 366.2 MiB)\n",
      "2023-10-30 18:26:00,472 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 9dc5d8af21bf:44443 in memory (size: 7.4 KiB, free: 366.2 MiB)\n",
      "2023-10-30 18:26:00,478 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 172.20.0.4:39963 in memory (size: 7.4 KiB, free: 366.2 MiB)\n",
      "2023-10-30 18:26:00,950 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2023-10-30 18:26:00,993 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2023-10-30 18:26:00,993 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "2023-10-30 18:26:03,640 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-10-30 18:26:03,641 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2023-10-30 18:26:03,746 INFO codegen.CodeGenerator: Code generated in 66.308169 ms\n",
      "2023-10-30 18:26:03,782 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 492.7 KiB, free 365.3 MiB)\n",
      "2023-10-30 18:26:03,794 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 54.6 KiB, free 365.2 MiB)\n",
      "2023-10-30 18:26:03,794 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 9dc5d8af21bf:44443 (size: 54.6 KiB, free: 366.2 MiB)\n",
      "2023-10-30 18:26:03,796 INFO spark.SparkContext: Created broadcast 6 from save at NativeMethodAccessorImpl.java:0\n",
      "2023-10-30 18:26:03,826 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-10-30 18:26:03,835 INFO scheduler.DAGScheduler: Registering RDD 16 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
      "2023-10-30 18:26:03,835 INFO scheduler.DAGScheduler: Got map stage job 4 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-10-30 18:26:03,835 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 5 (save at NativeMethodAccessorImpl.java:0)\n",
      "2023-10-30 18:26:03,835 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-10-30 18:26:03,835 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-10-30 18:26:03,838 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[16] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-10-30 18:26:03,860 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 30.6 KiB, free 365.2 MiB)\n",
      "2023-10-30 18:26:03,861 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 10.5 KiB, free 365.2 MiB)\n",
      "2023-10-30 18:26:03,862 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 9dc5d8af21bf:44443 (size: 10.5 KiB, free: 366.2 MiB)\n",
      "2023-10-30 18:26:03,863 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1535\n",
      "2023-10-30 18:26:03,863 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[16] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-10-30 18:26:03,863 INFO scheduler.TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
      "2023-10-30 18:26:03,865 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (172.20.0.4, executor 0, partition 0, PROCESS_LOCAL, 7950 bytes) \n",
      "2023-10-30 18:26:03,885 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.20.0.4:39963 (size: 10.5 KiB, free: 366.2 MiB)\n",
      "2023-10-30 18:26:03,971 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.20.0.4:39963 (size: 54.6 KiB, free: 366.2 MiB)\n",
      "2023-10-30 18:26:04,100 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 236 ms on 172.20.0.4 (executor 0) (1/1)\n",
      "2023-10-30 18:26:04,100 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "2023-10-30 18:26:04,105 INFO scheduler.DAGScheduler: ShuffleMapStage 5 (save at NativeMethodAccessorImpl.java:0) finished in 0.262 s\n",
      "2023-10-30 18:26:04,106 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-10-30 18:26:04,106 INFO scheduler.DAGScheduler: running: Set()\n",
      "2023-10-30 18:26:04,107 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-10-30 18:26:04,107 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-10-30 18:26:04,829 INFO parquet.ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "2023-10-30 18:26:04,870 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "2023-10-30 18:26:04,870 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2023-10-30 18:26:04,872 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "2023-10-30 18:26:04,872 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "2023-10-30 18:26:04,872 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2023-10-30 18:26:04,873 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "2023-10-30 18:26:05,457 INFO spark.SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\n",
      "2023-10-30 18:26:05,459 INFO scheduler.DAGScheduler: Got job 5 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-10-30 18:26:05,459 INFO scheduler.DAGScheduler: Final stage: ResultStage 7 (save at NativeMethodAccessorImpl.java:0)\n",
      "2023-10-30 18:26:05,459 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)\n",
      "2023-10-30 18:26:05,459 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-10-30 18:26:05,461 INFO scheduler.DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[18] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-10-30 18:26:05,516 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 310.5 KiB, free 364.9 MiB)\n",
      "2023-10-30 18:26:05,518 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 114.4 KiB, free 364.8 MiB)\n",
      "2023-10-30 18:26:05,519 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 9dc5d8af21bf:44443 (size: 114.4 KiB, free: 366.1 MiB)\n",
      "2023-10-30 18:26:05,519 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1535\n",
      "2023-10-30 18:26:05,520 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[18] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-10-30 18:26:05,520 INFO scheduler.TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
      "2023-10-30 18:26:05,522 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 5) (172.20.0.4, executor 0, partition 0, NODE_LOCAL, 7367 bytes) \n",
      "2023-10-30 18:26:05,540 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.20.0.4:39963 (size: 114.4 KiB, free: 366.1 MiB)\n",
      "2023-10-30 18:26:05,639 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.20.0.4:55392\n",
      "2023-10-30 18:26:11,085 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 7.0 (TID 5) in 5563 ms on 172.20.0.4 (executor 0) (1/1)\n",
      "2023-10-30 18:26:11,085 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "2023-10-30 18:26:11,087 INFO scheduler.DAGScheduler: ResultStage 7 (save at NativeMethodAccessorImpl.java:0) finished in 5.625 s\n",
      "2023-10-30 18:26:11,087 INFO scheduler.DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-10-30 18:26:11,087 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
      "2023-10-30 18:26:11,088 INFO scheduler.DAGScheduler: Job 5 finished: save at NativeMethodAccessorImpl.java:0, took 5.631002 s\n",
      "2023-10-30 18:26:11,095 INFO datasources.FileFormatWriter: Start to commit write Job a7d0082d-a177-4511-8cf0-7ddda8cd2d59.\n",
      "2023-10-30 18:26:11,307 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 9dc5d8af21bf:44443 in memory (size: 10.5 KiB, free: 366.1 MiB)\n",
      "2023-10-30 18:26:11,309 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 172.20.0.4:39963 in memory (size: 10.5 KiB, free: 366.1 MiB)\n",
      "2023-10-30 18:26:11,334 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 9dc5d8af21bf:44443 in memory (size: 114.4 KiB, free: 366.2 MiB)\n",
      "2023-10-30 18:26:11,341 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 172.20.0.4:39963 in memory (size: 114.4 KiB, free: 366.2 MiB)\n",
      "2023-10-30 18:26:11,906 INFO datasources.FileFormatWriter: Write Job a7d0082d-a177-4511-8cf0-7ddda8cd2d59 committed. Elapsed time: 808 ms.\n",
      "2023-10-30 18:26:11,909 INFO datasources.FileFormatWriter: Finished processing stats for write job a7d0082d-a177-4511-8cf0-7ddda8cd2d59.\n"
     ]
    }
   ],
   "source": [
    "df.limit(1000).write.format('parquet').mode('overwrite').save('s3a://spark-warehouse/raw_data/save_from_notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b77a47e-d976-471f-850d-a1de8a141da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-30 18:26:16,286 INFO spark.SparkContext: SparkContext is stopping with exitCode 0.\n",
      "2023-10-30 18:26:16,330 INFO server.AbstractConnector: Stopped Spark@2c536ca7{HTTP/1.1, (http/1.1)}{0.0.0.0:4051}\n",
      "2023-10-30 18:26:16,337 INFO ui.SparkUI: Stopped Spark web UI at http://9dc5d8af21bf:4051\n",
      "2023-10-30 18:26:16,346 INFO cluster.StandaloneSchedulerBackend: Shutting down all executors\n",
      "2023-10-30 18:26:16,347 INFO cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down\n",
      "2023-10-30 18:26:16,415 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "2023-10-30 18:26:16,443 INFO memory.MemoryStore: MemoryStore cleared\n",
      "2023-10-30 18:26:16,446 INFO storage.BlockManager: BlockManager stopped\n",
      "2023-10-30 18:26:16,457 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\n",
      "2023-10-30 18:26:16,472 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "2023-10-30 18:26:16,498 INFO spark.SparkContext: Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dbb345-dfc5-4dac-b4bd-0b66939a8776",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
